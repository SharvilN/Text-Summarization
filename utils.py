# -*- coding: utf-8 -*-
"""Seq2Seq Text Summarization With Attention

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fOqyIpXdbYcKD3FaJsfkMwWIFqhmYdjG
"""

from __future__ import absolute_import, division, print_function

# Import TensorFlow >= 1.10 and enable eager execution
import tensorflow as tf

import matplotlib.pyplot as plt

import re
import unicodedata
import numpy as np
import os
import pickle

MAX_LENGTH_ARTICLE = 400
MAX_LENGTH_SUMMARY = 100


def check_mismatch(articles, summaries):
    for article, summary in zip(articles, summaries):
        if article[: article.index('.')] != summary[: summary.index('.')]:
            raise ValueError("Mismatch in summary and its corresponding artcle file!")


def load_data(data_path):
    SENT = '.sent'
    SUMM = '.summ'

    data_map = {}

    articles, summaries = [], []

    for root, d_names, f_names in os.walk(data_path):
        for i, f in enumerate(f_names):
            if f.endswith(SENT):
                articles.append(os.path.join(root, f))
            elif f.endswith(SUMM):
                summaries.append(os.path.join(root, f))
            if f[:f.index('.')] in data_map.keys():
                data_map[f[:f.index('.')]] += 1
            else:
                data_map[f[:f.index('.')]] = 1

    ct = 0
    for key, value in data_map.items():
        if value == 1:
            ct += 1
            if get_path(data_path, key + SUMM) in summaries:
                summaries.remove(get_path(data_path, key + SUMM))
            else:
                articles.remove(get_path(data_path, key + SENT))
    print("Number of deleted articles/summaries: {}".format(ct))

    articles.sort()
    summaries.sort()

    check_mismatch(articles, summaries)

    print("Number of {} articles retrieved = ".format(data_path[data_path.rindex('/'):]), len(articles))
    print("Number of {} summaries retrieved = ".format(data_path[data_path.rindex('/'):]), len(summaries))

    return articles, summaries


def get_path(data_path, file):
    return os.path.join(data_path, file)


def get_text(file):
    with open(file) as f:
        text = f.read()

        return text


# Converts the unicode file to ascii
def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn')


def preprocess_article(w):

    w = unicode_to_ascii(w.lower().strip())
    # creating a space between a word and the punctuation following it
    # eg: "he is a boy." => "he is a boy ."
    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
    w = re.sub(r"([?.!,¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)

    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
    w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)

    w = w.rstrip().strip()

    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    w = '<start> ' + w + ' <end>'
    return w


def get_preprocessed_articles(articles, summaries):
    preprocessed_articles = [preprocess_article(get_text(file)) for file in articles]
    preprocessed_summaries = [preprocess_article(get_text(file)) for file in summaries]

    print(preprocessed_articles[:2])
    print(preprocessed_summaries[:2])

    return preprocessed_articles, preprocessed_summaries


# This class creates a word -> index mapping (e.g,. "dad" -> 5) and vice-versa
# (e.g., 5 -> "dad") for each language,
class Vocab():
    def __init__(self, text):
        self.text = text
        self.word2idx = {}
        self.idx2word = {}
        self.vocab = set()

        self.create_index()

    def create_index(self):
        for sentence in self.text:
            self.vocab.update(sentence.split(' '))

        self.vocab = sorted(self.vocab)

        self.word2idx['<PAD>'] = 0
        self.word2idx['<UNK'] = 1
        for index, word in enumerate(self.vocab):
            self.word2idx[word] = index + 2

        for word, index in self.word2idx.items():
            self.idx2word[index] = word


def build_vocab(articles, summaries):
    return Vocab(articles + summaries)


def build_tensors(vocab, articles, summaries):
    articles_tensor = [[vocab.word2idx[word] for word in sentence.split(' ')] for sentence in articles]
    summaries_tensor = [[vocab.word2idx[word] for word in sentence.split(' ')] for sentence in summaries]

    print(articles_tensor[0])
    print(summaries_tensor[0])

    return articles_tensor, summaries_tensor


def pad_tensors(articles_tensor, summaries_tensor, max_length_article=MAX_LENGTH_ARTICLE, max_length_summary=MAX_LENGTH_SUMMARY):
    # Padding the input and output tensor to the maximum length
    articles_tensor_padded = tf.keras.preprocessing.sequence.pad_sequences(articles_tensor,
                                                                           maxlen=max_length_article,
                                                                           padding='post')

    summaries_tensor_padded = tf.keras.preprocessing.sequence.pad_sequences(summaries_tensor,
                                                                            maxlen=max_length_summary,
                                                                            padding='post')
    print(articles_tensor_padded.shape)
    print(summaries_tensor_padded.shape)

    return articles_tensor_padded, summaries_tensor_padded


def save_tensor(tensor, file):
    # Write to file
    with open(file, 'wb') as f:
        pickle.dump(tensor, f)


def load_saved_tensor(path):
    with open(path, 'rb') as f:
        tensor = pickle.load(f)
    return tensor


def gru(units):
    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)
    # the code automatically does that.
    if tf.test.is_gpu_available():
        return tf.keras.layers.CuDNNGRU(units,
                                        return_sequences=True,
                                        return_state=True,
                                        recurrent_initializer='glorot_uniform')
    else:
        return tf.keras.layers.GRU(units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_activation='sigmoid',
                                   recurrent_initializer='glorot_uniform')


class Encoder(tf.keras.Model):
    def __init__(self, units, embedding_dim, vocab_size, batch_size):
        super(Encoder, self).__init__()
        self.enc_units = units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.batch_size = batch_size
        self.gru = gru(self.enc_units)

    def call(self, x, hidden):
        x = self.embedding(x)
        output, state = self.gru(x, initial_state=hidden)
        return output, state

    def initialize_hidden_state(self):
        return tf.zeros((self.batch_size, self.enc_units))


class Decoder(tf.keras.Model):
    def __init__(self, units, embedding_dim, vocab_size, batch_size):
        super(Decoder, self).__init__()
        self.dec_units = units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.batch_size = batch_size
        self.prediction_layer = tf.keras.layers.Dense(vocab_size)
        self.gru = gru(self.dec_units)
        # For attention
        self.W1 = tf.keras.layers.Dense(self.dec_units)
        self.W2 = tf.keras.layers.Dense(self.dec_units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, x, hidden, enc_output):
        # enc_output shape == (batch_size, max_length, hidden_size)
        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)

        hidden_with_time_axis = tf.expand_dims(hidden, 1)
        # score shape == (batch_size, max_length, 1)
        alpha = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))

        # attention_weights shape == (batch_size, max_length, 1)
        attention_weights = tf.nn.softmax(alpha, axis=1)

        # context_vector shape after summing == (batch_size, hidden_size)
        context_vector = attention_weights * enc_output
        context_vector = tf.reduce_sum(context_vector, axis=1)

        # x shape after passing through embedding == (batch_size, 1, embedding_dim)
        x = self.embedding(x)

        # x shape after concat == (batch_size, 1, hidden_size + embedding_dim)
        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)

        # passing vector to gru
        # why not pass context vector as hidden state instead of concatenating?
        output, state = self.gru(x)

        # output shape == (batch_size * 1, hidden_size)
        output = tf.reshape(output, (-1, output.shape[2]))
        # output shape == (batch_size * 1, vocab)
        x = self.prediction_layer(output)

        return x, state, attention_weights

    def initialize_hidden_state(self):
        return tf.zeros((self.batch_sz, self.dec_units))


def loss_function(real, pred):
    mask = 1 - np.equal(real, 0)
    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask
    return tf.reduce_mean(loss_)


# Evaluation
def evaluate(article, vocab, encoder, decoder, max_length_article, max_length_summary):

    attention_plot = np.zeros((max_length_summary, max_length_article))
    result = ''

    article_preprocessed = preprocess_article(article)
    inp_text = [vocab.word2idx[word] for word in article_preprocessed.split()]
    inputs = tf.keras.preprocessing.sequence.pad_sequences([inp_text],
                                                           maxlen=max_length_article,
                                                           padding='post')
    input_tensor = tf.convert_to_tensor(inputs)
    enc_output, enc_hidden = encoder(input_tensor)

    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([vocab.word2idx['<start>']], 0)

    for t in range(max_length_article):
        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)

        # storing the attention weigths to plot later on
        attention_weights = tf.reshape(attention_weights, (-1,))
        attention_plot[t] = attention_weights.numpy()

        predicted_id = tf.argmax(predictions[0]).numpy()

        result += vocab.idx2word[predicted_id] + ' '

        if vocab.idx2word[predicted_id] == '<end>':
            return result, article_preprocessed, attention_weights

        # the predicted ID is fed back into the model
        dec_input = tf.expand_dims([predicted_id], 0)

    return result, article_preprocessed, attention_plot


# function for plotting the attention weights
def plot_attention(attention, sentence, predicted_sentence):
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(1, 1, 1)
    ax.matshow(attention, cmap='viridis')

    fontdict = {'fontsize': 14}

    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

    plt.show()


def translate(sentence, encoder, decoder, max_length_inp, max_length_targ):
    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, max_length_inp,
                                                max_length_targ)

    print('Input: {}'.format(sentence))
    print('Predicted translation: {}'.format(result))

    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]
    plot_attention(attention_plot, sentence.split(' '), result.split(' '))
